{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functional Programming with Pure Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import jit, vmap, random\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "n_pies = 0\n",
    "def add_pies(pies_to_add: int):\n",
    "    return n_pies + pies_to_add\n",
    "\n",
    "print(jit(add_pies)(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_pies = 20\n",
    "print(jit(add_pies)(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def pure_add_pies(n_pies: int, pies_to_add: int):\n",
    "    return n_pies + pies_to_add\n",
    "\n",
    "print(jit(pure_add_pies)(0, 10))\n",
    "print(jit(pure_add_pies)(20, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the basics of immutability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100   2   3   4   5]\n",
      "This is not possible in JAX\n",
      "[1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "base = np.array([1, 2, 3, 4, 5])\n",
    "base[0] = 100\n",
    "print(base)\n",
    "# >>> [100, 2, 3, 4, 5]\n",
    "\n",
    "base_jax = jnp.array([1, 2, 3, 4, 5])\n",
    "try:\n",
    "    base_jax[0] = 100\n",
    "except:\n",
    "    print(\"This is not possible in JAX\")\n",
    "print(base_jax)\n",
    "# >>> TypeError: JAX arrays are immutable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100   2   3   4   5]\n",
      "[100   2   3   4   5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base = np.array([1, 2, 3, 4, 5])\n",
    "base[0] = 100\n",
    "print(base)\n",
    "# >>> [100, 2, 3, 4, 5]\n",
    "\n",
    "base_jax = jnp.array([1, 2, 3, 4, 5])\n",
    "# Notice the square brackets\n",
    "# and the assignment to the variable updated_base \n",
    "# we'll see why we need the assignment a bit later\n",
    "updated_base = base_jax.at[0].set(100)\n",
    "print(updated_base)\n",
    "# >>> [100, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding JIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.86 ms ± 60 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def selu(x, alpha=1.67, lambda_=1.05):\n",
    "  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = jnp.arange(1000000)\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426 µs ± 27.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "selu_jit = jax.jit(selu)\n",
    "\n",
    "# Warm up\n",
    "selu_jit(x).block_until_ready()\n",
    "\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VMAP explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is not possible in JAX\n"
     ]
    }
   ],
   "source": [
    "keygen = random.PRNGKey(0)\n",
    "weights = random.normal(keygen, shape=(20, 2)) # hidden layer with 20 neurons and 2 inputs\n",
    "features = random.normal(keygen, shape=(5, 2)) # batch of 5 samples with two features\n",
    "\n",
    "def dotproduct(w, x):\n",
    "    return jnp.dot(w, x) \n",
    "\n",
    "# dotproduct works on a single sample, but we want to apply it to all samples in the batch.\n",
    "dotproduct(10, 20) #works :)\n",
    "\n",
    "# now we want to apply it to all samples in the batch\n",
    "dotproduct = jit(dotproduct)\n",
    "try:\n",
    "    dotproduct(weights, features)\n",
    "except:\n",
    "    print(\"This is not possible in JAX\")\n",
    "# >>> TypeError: Incompatible shapes for dot: got (20, 2) and (5, 2).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jit\n",
    "def linear_forward(weights, features):\n",
    "    batch_dim_weights = None\n",
    "    batch_dim_features = 0\n",
    "    return vmap(dotproduct, in_axes=(batch_dim_weights, batch_dim_features))(weights, features)\n",
    "\n",
    "\n",
    "\n",
    "preds = linear_forward(weights, features)\n",
    "preds.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22801.0\n",
      "620.98846\n",
      "16.912619\n",
      "0.4606071\n",
      "0.012546455\n",
      "0.00034088735\n",
      "9.035924e-06\n",
      "3.541354e-07\n",
      "3.541354e-07\n",
      "3.541354e-07\n"
     ]
    }
   ],
   "source": [
    "from jax import grad\n",
    "w = jnp.array([20.])\n",
    "x = jnp.array([10.])\n",
    "b = jnp.array([1.])\n",
    "y_true = jnp.array([50.])\n",
    "alpha = 0.0001\n",
    "def forward(w, x, b):\n",
    "    return w * x + b\n",
    "\n",
    "def loss(y, y_true):\n",
    "    return jnp.mean((y - y_true) ** 2)\n",
    "for i in range(10000):\n",
    "    y_pred = forward(w, x, b)\n",
    "    \n",
    "    grad_w, grad_b = grad(loss, argnums=(0, 1))(y_pred, y_true)\n",
    "    w = w - alpha * grad_w\n",
    "    b = b - alpha * grad_b\n",
    "    if i % 1000 == 0:\n",
    "        print(loss(y_pred, y_true))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "846623b6fea914a09d8351c99a0b0d87ed0e842eab2c8bbb40644893e9b4a025"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
