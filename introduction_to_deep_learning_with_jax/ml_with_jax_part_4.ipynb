{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Connect, learn and contribute to help yourself and others land a job in the AI space\n",
    "\n",
    "Looking for a way to contribute or learn more about AI/ML, connect with me on medium:\n",
    "- LinkedIn: [https://www.linkedin.com/in/stefruinard/]()\n",
    "- Medium: [https://medium.com/@stefruinard]()\n",
    "- GitHub: [https://github.com/Sruinard]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning with JAX part IV\n",
    "Welcome to the fourth and final blog in this series on learning the fundamentals of JAX. Although all parts can be read seperately, you might want to check out some of the earlier blogs in this series, which you find [here](), [here]() and [here](). With that out of the way, welcome at SpaceY, we're thrilled to have you! After some intense onboarding SpaceY has set you up for the next challenge: Landing a rocket safely on the surface of the moon using reinforcement learning, JAX and [Haiku](https://dm-haiku.readthedocs.io/en/latest/). Are you ready for this next adventure? Great! Lets land some rockets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is reinforcement learning\n",
    "Our main goal will be to introduce you to the wonderful world of JAX (and Haiku in this case), but where is the fun in sticking to just that? So instead of training a model on the MNIST dataset, lets do something slightly more complex: Training a reinforcement learning agent to land a rocket on the moon. Reinforcement learning is an exciting 'new' area in the machine learning field. Although it still is mostly used in research, more and more fascinating applications containing reinforcement learning are deployed in practice: [RL at Deepmind](https://www.deepmind.com/blog/deep-reinforcement-learning) and [RL at Microsoft](https://blogs.microsoft.com/ai/reinforcement-learning/).\n",
    "\n",
    "In case you are not familiar with RL, here is the gist of it.\n",
    "In reinforcement learning we have an agent (sometimes referred to as a policy) which can take a set of actions in an environment. For example, you can have a game which is the environment and a player who can walk arount in that game environment as the agent. Everytime the agent takes an action, it 'observes' the environment and finds itself in a new state (you could also say it observes the state it is in). Now, given the action taken, it also receives a reward or a punishment (this is where the reinforcement part in reinforcement learning is coming from) which gives a signal to the agent about how happy to be with the action taken which closes the feedback loop. Alright, that are quite some moving components. We have Agents, Actions, Environment, Rewards, Observations/States. To make it slightly more intuitive, the following visual overview will probably help:\n",
    "\n",
    "![alt](https://github.com/Sruinard/machine_learning_novice/blob/main/assets/ml_with_jax_part_4/reinforcement_learning_concepts.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets map the terms we just introduced (e.g. agent, action, reward, state), to an overview containing terms you might be more familiar with:\n",
    "\n",
    "![](https://github.com/Sruinard/machine_learning_novice/blob/main/assets/ml_with_jax_part_4/reinforcement_learning_concepts_with_familiar_terms.png?raw=true)\n",
    "\n",
    "So what's next? What is actually that we try to achieve? Well, we want to maximize our rewards and use these rewards to train our neural network. So for that, we need some data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset in reinforcement learning. Not your typical dataset.\n",
    "\n",
    "In the previous parts we worked with a static dataset. For training our RL-agent we'll be generating a dataset through simulation. Each time our agent interacts with the environment, we'll register the current state, the action, the next state, the reward and whether it is the end of the game. We'll save this to the agent's memory which we set to 10000 (you'll see this later in code). What will happen is that once the agents learns more and more about the environment, our experiences will include higher rewards from which the agent can learn again. It's basically teaching itself. Be aware that since the agent's memory is limited, older experiences are pushed out and replaced by the new experiences. ![Building our dataset](../assets/ml_with_jax_part_4/dataset_rl.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/haiku/_src/data_structures.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jaxlib\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import haiku as hk\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the environment and inspecting the action space\n",
    "We'll leverage the gym package created by [OpenAI](https://openai.com/). In our case we'll use it to create the LunarLander environment with which our RL-agent will interact. So lets do that! Lets create the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2', new_step_api=True)\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy as you like. There are four discrete actions available: 1) do nothing, 2) fire left orientation engine, 3) fire main engine, 4)fire right orientation engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we (still) need\n",
    "\n",
    "- [x] Environment \n",
    "- [] Memory Buffer\n",
    "- [] DQN model\n",
    "- [] loss function\n",
    "- [] Training Loop\n",
    "\n",
    "With the environment created, lets move forward with creating the memory buffer. We'll first create a training configuration which is used throughout this notebook. For now, pay particular attention to the memory size. This will be the maximum amount of experiences we'll hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig:\n",
    "    MEMORY_SIZE = 10000\n",
    "    BATCH_SIZE = 32\n",
    "    UPDATE_PARAMS_EVERY_N_STEPS = 4\n",
    "    GAMMA = 0.995\n",
    "    TAU = 0.001\n",
    "    E_MIN = 0.01\n",
    "    E_DECAY = 0.995\n",
    "    N_EPISODES = 900\n",
    "    MAX_N_STEPS_PER_EPISODE = 1000\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll create the actual memory. A simple queue in which we stare Experiences. The experiences is a single sample and when we take multiple samples from our queue, they'll form a batch used for training. The experience dataclass contains all the information to train the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from typing import NamedTuple\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Experience:\n",
    "    state: jnp.ndarray\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: jnp.ndarray\n",
    "    done: bool\n",
    "\n",
    "\n",
    "memory = deque(maxlen=TrainConfig.MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we (still) need\n",
    "\n",
    "- [x] Environment \n",
    "- [x] Memory Buffer\n",
    "- [] DQN model\n",
    "- [] loss function\n",
    "- [] Training Loop\n",
    "\n",
    "Next it is time to start creating our model and learn more about Jax and Haiku. As always, we'll create a training state as we have to deal with a functional programming paradigm. The major difference this time is that we will not create one, but two(!) networks. This is done to make learning more stable. What we'll happen is that at every `TrainConfig.UPDATE_PARAMS_EVERY_N_STEPS` our params are updated using gradient descent. Next, we'll define the model `def network_fn()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class TrainingState(NamedTuple):\n",
    "    params: hk.Params\n",
    "    target_params: hk.Params\n",
    "    eval_params: hk.Params\n",
    "    opt_state: optax.OptState\n",
    "\n",
    "class Batch(NamedTuple):\n",
    "    states: jnp.ndarray\n",
    "    actions: int\n",
    "    rewards: float\n",
    "    next_states: jnp.ndarray\n",
    "    dones: bool\n",
    "\n",
    "\n",
    "\n",
    "def network_fn(x: jnp.ndarray) -> jnp.ndarray:\n",
    "    model = hk.Sequential(\n",
    "        [\n",
    "            hk.Linear(64),jax.nn.relu,\n",
    "            hk.Linear(64), jax.nn.relu,\n",
    "            hk.Linear(NUM_ACTIONS),\n",
    "        ]\n",
    "\n",
    "    )\n",
    "    return model(x)\n",
    "\n",
    "network = hk.without_apply_rng(hk.transform(network_fn))\n",
    "target_network = hk.without_apply_rng(hk.transform(network_fn))\n",
    "optimiser = optax.adam(1e-3)\n",
    "# Initialise network and optimiser; note we draw an input to get shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to initialize, we need to get some data to work with. We create a helper function to sample a batch of data from memory. Next we are going to play around in the environment taking random actions and store those experiences in our memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.random as jrandom\n",
    "import random\n",
    "import numpy as np\n",
    "keygen = jrandom.PRNGKey(0)\n",
    "\n",
    "def get_random_batch(memory):\n",
    "    batch = random.sample(memory, k=TrainConfig.BATCH_SIZE)\n",
    "    return Batch(\n",
    "        states=jnp.array([e.state for e in batch]),\n",
    "        actions=jnp.array([e.action for e in batch]),\n",
    "        rewards=jnp.array([e.reward for e in batch]),\n",
    "        next_states=jnp.array([e.next_state for e in batch]),\n",
    "        dones=jnp.array([e.done for e in batch]),\n",
    "    )\n",
    "\n",
    "small_memory = deque(maxlen=1000)\n",
    "\n",
    "state = env.reset()\n",
    "action = env.action_space.sample()\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, is_done, *_ = env.step(action)\n",
    "    experience = Experience(state, action, reward, next_state, is_done)\n",
    "    small_memory.append(experience)\n",
    "\n",
    "batch = get_random_batch(small_memory)\n",
    "\n",
    "batch.states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a batch of experiences, we can initialize the networks. Note again that our training state contains the parameters of two networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.\n",
      "  leaves, treedef = jax.tree_flatten(tree)\n",
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/haiku/_src/data_structures.py:145: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.\n",
      "  return jax.tree_unflatten(treedef, leaves)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "initial_params = network.init(\n",
    "    jax.random.PRNGKey(seed=0), batch.states)\n",
    "initial_target_params = target_network.init(jax.random.PRNGKey(seed=1), batch.states)\n",
    "initial_opt_state = optimiser.init(initial_params)\n",
    "train_state = TrainingState(initial_params, initial_target_params, initial_params, initial_opt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we (still) need\n",
    "\n",
    "- [x] Environment \n",
    "- [x] Memory Buffer\n",
    "- [x] DQN model\n",
    "- [] loss function\n",
    "- [] Training Loop\n",
    "\n",
    "Aaaah the good old loss function. In the previous blogs we were dealing with supervised learning, which meant we had some labels. That's not the case anymore. We have to define our targets ourself. \n",
    "\n",
    "The target values are given by the Bellman equation:\n",
    "\n",
    "$$\n",
    "y = R + \\gamma \\max_{a'}Q(s',a';w)\n",
    "$$\n",
    "\n",
    "where $\\gamma$ impacts whether the agent focuses on the long term (when close to 1) or short term rewards (close to 0), and $w$ are the weights of the neural network. By using the target network, the loss becomes:\n",
    "\n",
    "$$\n",
    "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}\\hat{Q}(s',a'; w^{target})}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
    "$$\n",
    "\n",
    "where $w^{target}$ and $w$ are the weights of the target network and $w$ of the primary network, respectively.\n",
    "\n",
    "Finally, we update the weights gently (or in a soft fashion). This means that the weights of the target networks are updated by a weighted average of the original neural network and the target neural network.\n",
    " \n",
    "$$\n",
    "w^{target}\\leftarrow \\tau w + (1 - \\tau) w^{target}\n",
    "$$\n",
    "\n",
    "where $\\tau$ is normally close to 0. By using the soft update, we are ensuring that the target values, $y$, change slowly, which improves the stability of our learning algorithm.\n",
    "\n",
    "Lets implement it in code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
    "params = train_state.params\n",
    "state_actions_values = network.apply(params, batch.next_states)\n",
    "max_state_actions_values = jnp.max(state_actions_values, axis=1)\n",
    "targets = batch.rewards + TrainConfig.GAMMA * jnp.where(batch.dones, 0.0, max_state_actions_values)\n",
    "\n",
    "q_values = network.apply(params, batch.states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "q_value_for_action_taken = q_values[jnp.arange(q_values.shape[0]), batch.actions]\n",
    "q_value_for_action_taken.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2518773  0.20313816 0.13113081 0.04511986]\n",
      "1\n",
      "0.20313816\n"
     ]
    }
   ],
   "source": [
    "print(q_values[0])\n",
    "print(batch.actions[0])\n",
    "print(q_value_for_action_taken[0])\n",
    "assert q_value_for_action_taken[0] == q_values[0][batch.actions[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a closer look at the inline comments as they explain more closely the meaning of each matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, target_params, batch):\n",
    "    # q_values: expected future reward for taking an action when in a given state\n",
    "    # i.e. shape = (32, 4)\n",
    "    q_values = network.apply(params, batch.states)\n",
    "    # q_values: expected future reward for actual action taken for each sample\n",
    "    # i.e. shape = (32,)\n",
    "    q_values_pred = q_values[jnp.arange(q_values.shape[0]), batch.actions]\n",
    "\n",
    "    # q_values_next: expected future reward for taking an action when in the next state\n",
    "    # i.e. shape = (32, 4)\n",
    "    q_values_next = target_network.apply(target_params, batch.next_states)\n",
    "    # q_values_next: expected future reward for taking an action when in the next state\n",
    "    # i.e. shape = (32,)\n",
    "    q_values_next_max = jnp.max(q_values_next, axis=1)\n",
    "\n",
    "    # build the target\n",
    "    q_value_true = batch.rewards + TrainConfig.GAMMA * jnp.where(batch.dones, 0.0, q_values_next_max)\n",
    "\n",
    "    # compute the loss\n",
    "    return jnp.mean((q_values_pred - q_value_true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have to update our parameters of both the target network and the original network using gradient descent. The function `optax.incremental_update()` is used to apply a soft update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(train_state: TrainingState, batch: Batch) -> TrainingState:\n",
    "    \"\"\"Learning rule (stochastic gradient descent).\"\"\"\n",
    "    grads = jax.grad(loss)(train_state.params, train_state.target_params, batch)\n",
    "    updates, opt_state = optimiser.update(grads, train_state.opt_state)\n",
    "    params = optax.apply_updates(train_state.params, updates)\n",
    "\n",
    "    # Update target network.\n",
    "    # params * TAU + (1 - TAU) * new_params\n",
    "    # target_params = params * TrainConfig.TAU  + (1 - TrainConfig.TAU) * train_state.target_params\n",
    "    target_params = optax.incremental_update(params, train_state.target_params, TrainConfig.TAU)\n",
    "    \n",
    "    # Compute avg_params, the exponential moving average of the \"live\" params.\n",
    "    # We use this only for evaluation (cf. https://doi.org/10.1137/0330046).\n",
    "    eval_params = optax.incremental_update(\n",
    "        params, train_state.eval_params, step_size=0.001)\n",
    "    return TrainingState(params, target_params, eval_params, opt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that covered, the final thing we need to do is create the training loop. We'll use some more tricks and tricks to make training work better, but you can skip that if you're not interested in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.27653083, -0.02404503,  0.06786434,  0.0538235 ],\n",
       "             [ 0.27653083, -0.02404503,  0.06786434,  0.0538235 ]],            dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "batch_state = jnp.array([state, state])\n",
    "network.apply(train_state.params, state)\n",
    "network.apply(train_state.params, batch_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_epsilon(epsilon, train_config: TrainConfig):\n",
    "    return max(train_config.E_MIN, train_config.E_DECAY*epsilon)\n",
    "\n",
    "def exploit_or_explore(q_value: jnp.ndarray, epsilon: float = 0.1) -> int:\n",
    "    \"\"\"Exploit or explore according to epsilon-greedy policy.\"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.array(jnp.argmax(q_value))\n",
    "\n",
    "def is_update_params(n_steps_taken: int, train_config: TrainConfig) -> bool:\n",
    "    \"\"\"Update params every `update_params_every` steps.\"\"\"\n",
    "    return (n_steps_taken + 1) % train_config.UPDATE_PARAMS_EVERY_N_STEPS == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we (still) need\n",
    "\n",
    "- [x] Environment \n",
    "- [x] Memory Buffer\n",
    "- [x] DQN model\n",
    "- [x] loss function\n",
    "- [] Training Loop\n",
    "\n",
    "Here is what will happen. We initialize some variables for keeping track of metrics and to create some cool visualizations later on.\n",
    "\n",
    "Then we'll specify the number of episodes we'll let our agent train. Our agent will take a maximum of `train_config.MAX_N_STEPS_PER_EPISODE` or savely lands the moonlander before that number of steps is reached. The experience is added to the agents memory and if all conditions for updating the params are met (i.e. `if is_update_params()`), we'll update our parameters using gradient descent. We are confident we can safely land the moonlander if we score an average reward of 200 over more than 100 episodes. Lets see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 | Total point average of the last 100 episodes: -115.95\n",
      "Episode 191 | Total point average of the last 100 episodes: -42.267"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/ml_with_jax_part_4.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/ml_with_jax_part_4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m total_reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/ml_with_jax_part_4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(train_config\u001b[39m.\u001b[39mMAX_N_STEPS_PER_EPISODE):\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/ml_with_jax_part_4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     q_value \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39;49mapply(train_state\u001b[39m.\u001b[39;49mparams, state)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/ml_with_jax_part_4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     action \u001b[39m=\u001b[39m exploit_or_explore(q_value\u001b[39m=\u001b[39mq_value, epsilon\u001b[39m=\u001b[39mepsilon)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/ml_with_jax_part_4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     next_state, reward, is_done, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/haiku/_src/multi_transform.py:298\u001b[0m, in \u001b[0;36mwithout_apply_rng.<locals>.apply_fn\u001b[0;34m(params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_fn\u001b[39m(params, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    297\u001b[0m   check_rng_kwarg(kwargs)\n\u001b[0;32m--> 298\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39;49mapply(params, \u001b[39mNone\u001b[39;49;00m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/haiku/_src/transform.py:127\u001b[0m, in \u001b[0;36mwithout_state.<locals>.apply_fn\u001b[0;34m(params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    121\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mHaiku transform adds three arguments (params, state, rng) to apply. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mIf the functions you are transforming use the same names you must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mpass them positionally (e.g. `f.apply(.., my_state)` and not by \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mname (e.g. `f.apply(.., state=my_state)`)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m out, state \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mapply(params, {}, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    128\u001b[0m \u001b[39mif\u001b[39;00m state:\n\u001b[1;32m    129\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf your transformed function uses `hk.\u001b[39m\u001b[39m{\u001b[39m\u001b[39mget,set}_state` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39mthen use `hk.transform_with_state`.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/haiku/_src/transform.py:354\u001b[0m, in \u001b[0;36mtransform_with_state.<locals>.apply_fn\u001b[0;34m(params, state, rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mwith\u001b[39;00m base\u001b[39m.\u001b[39mnew_context(params\u001b[39m=\u001b[39mparams, state\u001b[39m=\u001b[39mstate, rng\u001b[39m=\u001b[39mrng) \u001b[39mas\u001b[39;00m ctx:\n\u001b[1;32m    353\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m     out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    355\u001b[0m   \u001b[39mexcept\u001b[39;00m jax\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mUnexpectedTracerError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    356\u001b[0m     \u001b[39mraise\u001b[39;00m jax\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mUnexpectedTracerError(unexpected_tracer_hint) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;32m/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/ml_with_jax_part_4.ipynb Cell 34\u001b[0m in \u001b[0;36mnetwork_fn\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/ml_with_jax_part_4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnetwork_fn\u001b[39m(x: jnp\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m jnp\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/ml_with_jax_part_4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     model \u001b[39m=\u001b[39m hk\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/ml_with_jax_part_4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m         [\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/ml_with_jax_part_4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m             hk\u001b[39m.\u001b[39mLinear(\u001b[39m64\u001b[39m),jax\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mrelu,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/ml_with_jax_part_4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m             hk\u001b[39m.\u001b[39mLinear(\u001b[39m64\u001b[39m), jax\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mrelu,\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/ml_with_jax_part_4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m             hk\u001b[39m.\u001b[39;49mLinear(NUM_ACTIONS),\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/ml_with_jax_part_4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m         ]\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/ml_with_jax_part_4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/ml_with_jax_part_4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/ml_with_jax_part_4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model(x)\n",
      "File \u001b[0;32m/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/haiku/_src/module.py:125\u001b[0m, in \u001b[0;36mModuleMetaclass.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m init(module, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m (config\u001b[39m.\u001b[39mget_config()\u001b[39m.\u001b[39mmodule_auto_repr \u001b[39mand\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[39mgetattr\u001b[39m(module, \u001b[39m\"\u001b[39m\u001b[39mAUTO_REPR\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)):\n\u001b[0;32m--> 125\u001b[0m   module\u001b[39m.\u001b[39m_auto_repr \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mauto_repr(\u001b[39mcls\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m   module\u001b[39m.\u001b[39m_auto_repr \u001b[39m=\u001b[39m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__repr__\u001b[39m(module)\n",
      "File \u001b[0;32m/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/haiku/_src/utils.py:66\u001b[0m, in \u001b[0;36mauto_repr\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mauto_repr\u001b[39m(\u001b[39mcls\u001b[39m: Type[Any], \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     42\u001b[0m   \u001b[39m\"\"\"Derives a `__repr__` from constructor arguments of a given class.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[39m      >>> class Foo:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m    A string representing a call equivalent to `cls(*args, **kwargs)`.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m   argspec \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39;49mgetfullargspec(\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m)\n\u001b[1;32m     67\u001b[0m   arg_names \u001b[39m=\u001b[39m argspec\u001b[39m.\u001b[39margs\n\u001b[1;32m     68\u001b[0m   \u001b[39m# Keep used positionals minus self.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/python/3.10.4/lib/python3.10/inspect.py:1306\u001b[0m, in \u001b[0;36mgetfullargspec\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1303\u001b[0m defaults \u001b[39m=\u001b[39m ()\n\u001b[1;32m   1304\u001b[0m kwdefaults \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1306\u001b[0m \u001b[39mif\u001b[39;00m sig\u001b[39m.\u001b[39mreturn_annotation \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m sig\u001b[39m.\u001b[39mempty:\n\u001b[1;32m   1307\u001b[0m     annotations[\u001b[39m'\u001b[39m\u001b[39mreturn\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m sig\u001b[39m.\u001b[39mreturn_annotation\n\u001b[1;32m   1309\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m sig\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mvalues():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params_at_different_training_steps = {}\n",
    "total_reward_history = []\n",
    "moving_average_window_size = 100\n",
    "epsilon = 1.0\n",
    "train_config = TrainConfig()\n",
    "for episode in range(train_config.N_EPISODES):\n",
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for step in range(train_config.MAX_N_STEPS_PER_EPISODE):\n",
    "        q_value = network.apply(train_state.params, state)\n",
    "        action = exploit_or_explore(q_value=q_value, epsilon=epsilon)\n",
    "        \n",
    "        next_state, reward, is_done, *_ = env.step(action)\n",
    "        experience = Experience(state, action, reward, next_state, is_done)\n",
    "        memory.append(experience)\n",
    "        if len(memory) < TrainConfig.MEMORY_SIZE:\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        if is_update_params(step, train_config=train_config):\n",
    "            batch = get_random_batch(memory)\n",
    "            train_state = update(train_state, batch)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if is_done:\n",
    "            break\n",
    "\n",
    "    total_reward_history.append(total_reward)\n",
    "    mean_total_reward_in_window = np.mean(total_reward_history[-moving_average_window_size:])\n",
    "    epsilon = update_epsilon(epsilon, train_config)\n",
    "\n",
    "\n",
    "    print(f\"\\rEpisode {episode+1} | Total point average of the last {moving_average_window_size} episodes: {mean_total_reward_in_window:.2f}\", end=\"\")\n",
    "\n",
    "    if (episode+1) % moving_average_window_size == 0:\n",
    "        print(f\"\\rEpisode {episode+1} | Total point average of the last {moving_average_window_size} episodes: {mean_total_reward_in_window:.2f}\")\n",
    "\n",
    "    if (episode+1) % 100 == 0:\n",
    "        network_params_name = f\"params_episode_{episode + 1}\"\n",
    "        params_at_different_training_steps[network_params_name] = train_state.params\n",
    "\n",
    "    # We will consider that the environment is solved if we get an\n",
    "    # average of 200 points in the last 100 episodes.\n",
    "    if mean_total_reward_in_window >= 200.0:\n",
    "        print(f\"\\n\\nEnvironment solved in {episode+1} episodes!\")\n",
    "        network_params_name = f\"params_episode_final\"\n",
    "        params_at_different_training_steps[network_params_name] = train_state.params\n",
    "print(\"done training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create some cool visuals to see how our agent improves over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "\n",
    "def create_video(filename, env, train_state, fps=30):\n",
    "    max_steps = 300\n",
    "    steps = 0\n",
    "    with imageio.get_writer(filename, fps=fps) as video:\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        frame = env.render(mode=\"rgb_array\")\n",
    "        video.append_data(frame)\n",
    "        while not done:    \n",
    "            steps +=1\n",
    "            q_values = network.apply(train_state.params, state)\n",
    "            action = jnp.argmax(q_values)\n",
    "            state, _, done, *_ = env.step(np.asarray([action])[0])\n",
    "            frame = env.render(mode=\"rgb_array\")\n",
    "            video.append_data(frame)\n",
    "            if done and steps < max_steps:\n",
    "                while steps < max_steps:\n",
    "                    video.append_data(frame)\n",
    "                    steps += 1\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    os.makedirs('./videos')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "for episode_name, episode_params in params_at_different_training_steps.items():\n",
    "    filename = f\"./videos/lunar_{episode_name}.gif\"\n",
    "    episode_train_state = TrainingState(episode_params, episode_params, episode_params, initial_opt_state)\n",
    "    create_video(filename, env, episode_train_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| | | |\n",
    "|:-------------------------:|:-------------------------:|:-------------------------:|\n",
    "|<img width=\"1604\" alt=\"screen shot 2017-08-07 at 12 18 15 pm\" src=\"../assets/videos/lunar_params_episode_100.gif\">  100 Episodes |  <img width=\"1604\" alt=\"screen shot 2017-08-07 at 12 18 15 pm\" src=\"../assets/videos/lunar_params_episode_200.gif\"> 200 Episodes|<img width=\"1604\" alt=\"screen shot 2017-08-07 at 12 18 15 pm\" src=\"../assets/videos/lunar_params_episode_300.gif\"> 300 Episodes |\n",
    "|<img width=\"1604\" alt=\"screen shot 2017-08-07 at 12 18 15 pm\" src=\"../assets/videos/lunar_params_episode_400.gif\">  400 Episodes |  <img width=\"1604\" alt=\"screen shot 2017-08-07 at 12 18 15 pm\" src=\"../assets/videos/lunar_params_episode_500.gif\"> 500 Episodes|<img width=\"1604\" alt=\"screen shot 2017-08-07 at 12 18 15 pm\" src=\"../assets/videos/lunar_params_episode_600.gif\"> 600 Episodes |\n",
    "|<img width=\"1604\" alt=\"screen shot 2017-08-07 at 12 18 15 pm\" src=\"../assets/videos/lunar_params_episode_700.gif\">  700 Episodes |  <img width=\"1604\" alt=\"screen shot 2017-08-07 at 12 18 15 pm\" src=\"../assets/videos/lunar_params_episode_800.gif\"> 800 Episodes|<img width=\"1604\" alt=\"screen shot 2017-08-07 at 12 18 15 pm\" src=\"../assets/videos/lunar_params_episode_900.gif\"> 900 Episodes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Well done if you made it this far! You just landed a man (uhhh) moonlander on the moon. This was the final blogpost in this series on getting started with Jax. In this blog we learnt a bit about Reinforcement Learning and how to use Jax to train an RL-agent. Pretty awesome! Stay tuned for more advanced tutorials and how to run your deep learning models in production!\n",
    "\n",
    "Finally, feel free to decide for yourself if you want to, but if you liked this blogpost (or the series in general), I'd greatly appreciate if you could like it on Medium and/or GitHub to help others more easily find this content as well.\n",
    "\n",
    "## Connect, learn and contribute to help yourself and others land a job in the AI space\n",
    "\n",
    "Looking for a way to contribute or learn more about AI/ML, connect with me on medium:\n",
    "- LinkedIn: [https://www.linkedin.com/in/stefruinard/](https://www.linkedin.com/in/stefruinard/)\n",
    "- Medium: [https://medium.com/@stefruinard](https://medium.com/@stefruinard)\n",
    "- GitHub: [https://github.com/Sruinard](https://github.com/Sruinard)\n",
    "\n",
    "## Contributors:\n",
    "###### Submit a Pull Request or reach out on LinkedIn and become a recognized contributor :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fd19f60d2496e9c9e375be8bbaf07fa2fff2a8edc3dc6611f1c0b323d41b84b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
