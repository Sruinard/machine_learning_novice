{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/haiku/_src/data_structures.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jaxlib\n",
    "import optax\n",
    "import haiku as hk\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2', new_step_api=True)\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we need\n",
    "\n",
    "- [x] Environment \n",
    "- [] Memory Buffer\n",
    "- [] DQN model\n",
    "- [] loss function\n",
    "- [] Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig:\n",
    "    MEMORY_SIZE = 10000\n",
    "    BATCH_SIZE = 32\n",
    "    UPDATE_PARAMS_EVERY_N_STEPS = 4\n",
    "    GAMMA = 0.995\n",
    "    TAU = 0.001\n",
    "    E_MIN = 0.01\n",
    "    E_DECAY = 0.995\n",
    "    N_EPISODES = 1000\n",
    "    MAX_N_STEPS_PER_EPISODE = 1000\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from typing import NamedTuple\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Experience:\n",
    "    state: jnp.ndarray\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: jnp.ndarray\n",
    "    done: bool\n",
    "\n",
    "\n",
    "memory = deque(maxlen=TrainConfig.MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we need\n",
    "\n",
    "- [x] Environment \n",
    "- [x] Memory Buffer\n",
    "- [] DQN model\n",
    "- [] loss function\n",
    "- [] Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "# @dataclasses.dataclass\n",
    "class TrainingState(NamedTuple):\n",
    "    params: hk.Params\n",
    "    target_params: hk.Params\n",
    "    eval_params: hk.Params\n",
    "    opt_state: optax.OptState\n",
    "\n",
    "# @dataclasses.dataclass\n",
    "class Batch(NamedTuple):\n",
    "    states: jnp.ndarray\n",
    "    actions: int\n",
    "    rewards: float\n",
    "    next_states: jnp.ndarray\n",
    "    dones: bool\n",
    "\n",
    "\n",
    "\n",
    "def network_fn(x: jnp.ndarray) -> jnp.ndarray:\n",
    "    model = hk.Sequential(\n",
    "        [\n",
    "            hk.Linear(64),jax.nn.relu,\n",
    "            hk.Linear(64), jax.nn.relu,\n",
    "            hk.Linear(NUM_ACTIONS),\n",
    "        ]\n",
    "\n",
    "    )\n",
    "    return model(x)\n",
    "\n",
    "network = hk.without_apply_rng(hk.transform(network_fn))\n",
    "target_network = hk.without_apply_rng(hk.transform(network_fn))\n",
    "optimiser = optax.adam(1e-3)\n",
    "# Initialise network and optimiser; note we draw an input to get shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.random as jrandom\n",
    "import random\n",
    "import numpy as np\n",
    "keygen = jrandom.PRNGKey(0)\n",
    "\n",
    "def get_random_batch(memory):\n",
    "    batch = random.sample(memory, k=TrainConfig.BATCH_SIZE)\n",
    "    return Batch(\n",
    "        states=jnp.array([e.state for e in batch]),\n",
    "        actions=jnp.array([e.action for e in batch]),\n",
    "        rewards=jnp.array([e.reward for e in batch]),\n",
    "        next_states=jnp.array([e.next_state for e in batch]),\n",
    "        dones=jnp.array([e.done for e in batch]),\n",
    "    )\n",
    "\n",
    "small_memory = deque(maxlen=1000)\n",
    "\n",
    "state = env.reset()\n",
    "action = env.action_space.sample()\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, is_done, *_ = env.step(action)\n",
    "    experience = Experience(state, action, reward, next_state, is_done)\n",
    "    small_memory.append(experience)\n",
    "\n",
    "batch = get_random_batch(small_memory)\n",
    "\n",
    "batch.states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.\n",
      "  leaves, treedef = jax.tree_flatten(tree)\n",
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/haiku/_src/data_structures.py:145: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.\n",
      "  return jax.tree_unflatten(treedef, leaves)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "initial_params = network.init(\n",
    "    jax.random.PRNGKey(seed=0), batch.states)\n",
    "initial_opt_state = optimiser.init(initial_params)\n",
    "train_state = TrainingState(initial_params, initial_params, initial_params, initial_opt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we need\n",
    "\n",
    "- [x] Environment \n",
    "- [x] Memory Buffer\n",
    "- [x] DQN model\n",
    "- [] loss function\n",
    "- [] Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
    "params = train_state.params\n",
    "state_actions_values = network.apply(params, batch.next_states)\n",
    "max_state_actions_values = jnp.max(state_actions_values, axis=1)\n",
    "targets = batch.rewards + TrainConfig.GAMMA * jnp.where(batch.dones, 0.0, max_state_actions_values)\n",
    "\n",
    "q_values = network.apply(params, batch.states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "q_value_for_action_taken = q_values[jnp.arange(q_values.shape[0]), batch.actions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3777553   0.03897272  0.13142307 -0.01228983]\n",
      "0\n",
      "0.3777553\n"
     ]
    }
   ],
   "source": [
    "print(q_values[0])\n",
    "print(batch.actions[0])\n",
    "print(q_value_for_action_taken[0])\n",
    "assert q_value_for_action_taken[0] == q_values[0][batch.actions[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, target_params, batch):\n",
    "    q_values = network.apply(params, batch.states)\n",
    "    q_values_pred = q_values[jnp.arange(q_values.shape[0]), batch.actions]\n",
    "\n",
    "    q_values_next = target_network.apply(target_params, batch.next_states)\n",
    "    q_values_next_max = jnp.max(q_values_next, axis=1)\n",
    "\n",
    "    q_value_true = batch.rewards + TrainConfig.GAMMA * jnp.where(batch.dones, 0.0, q_values_next_max)\n",
    "    return jnp.mean((q_values_pred - q_value_true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(train_state: TrainingState, batch: Batch) -> TrainingState:\n",
    "    \"\"\"Learning rule (stochastic gradient descent).\"\"\"\n",
    "    grads = jax.grad(loss)(train_state.params, train_state.target_params, batch)\n",
    "    updates, opt_state = optimiser.update(grads, train_state.opt_state)\n",
    "    params = optax.apply_updates(train_state.params, updates)\n",
    "\n",
    "    # Update target network.\n",
    "    # params * TAU + (1 - TAU) * new_params\n",
    "    # target_params = params * TrainConfig.TAU  + (1 - TrainConfig.TAU) * train_state.target_params\n",
    "    target_params = optax.incremental_update(params, train_state.target_params, TrainConfig.TAU)\n",
    "    \n",
    "    # Compute avg_params, the exponential moving average of the \"live\" params.\n",
    "    # We use this only for evaluation (cf. https://doi.org/10.1137/0330046).\n",
    "    eval_params = optax.incremental_update(\n",
    "        params, train_state.eval_params, step_size=0.001)\n",
    "    return TrainingState(params, target_params, eval_params, opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.30043876, 0.21810007, 0.20305528, 0.05785726],\n",
       "             [0.30043876, 0.21810007, 0.20305528, 0.05785726]],            dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "batch_state = jnp.array([state, state])\n",
    "network.apply(train_state.params, state)\n",
    "network.apply(train_state.params, batch_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_epsilon(epsilon, train_config: TrainConfig):\n",
    "    return max(train_config.E_MIN, train_config.E_DECAY*epsilon)\n",
    "\n",
    "def exploit_or_explore(q_value: jnp.ndarray, epsilon: float = 0.1) -> int:\n",
    "    \"\"\"Exploit or explore according to epsilon-greedy policy.\"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.array(jnp.argmax(q_value))\n",
    "\n",
    "def is_update_params(n_steps_taken: int, train_config: TrainConfig) -> bool:\n",
    "    \"\"\"Update params every `update_params_every` steps.\"\"\"\n",
    "    return (n_steps_taken + 1) % train_config.UPDATE_PARAMS_EVERY_N_STEPS == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 | Total point average of the last 100 episodes: -170.87\n",
      "Episode 200 | Total point average of the last 100 episodes: -147.81\n",
      "Episode 300 | Total point average of the last 100 episodes: -39.286\n",
      "Episode 400 | Total point average of the last 100 episodes: 40.056\n",
      "Episode 500 | Total point average of the last 100 episodes: 94.68\n",
      "Episode 600 | Total point average of the last 100 episodes: 144.83\n",
      "Episode 664 | Total point average of the last 100 episodes: 178.36"
     ]
    }
   ],
   "source": [
    "params_at_different_training_steps = {}\n",
    "total_reward_history = []\n",
    "moving_average_window_size = 100\n",
    "epsilon = 1.0\n",
    "train_config = TrainConfig()\n",
    "for episode in range(train_config.N_EPISODES):\n",
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for step in range(train_config.MAX_N_STEPS_PER_EPISODE):\n",
    "        q_value = network.apply(train_state.params, state)\n",
    "        action = exploit_or_explore(q_value=q_value, epsilon=epsilon)\n",
    "        \n",
    "        next_state, reward, is_done, *_ = env.step(action)\n",
    "        experience = Experience(state, action, reward, next_state, is_done)\n",
    "        memory.append(experience)\n",
    "        if len(memory) < TrainConfig.MEMORY_SIZE:\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        if is_update_params(step, train_config=train_config):\n",
    "            batch =get_random_batch(memory)\n",
    "            train_state = update(train_state, batch)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if is_done:\n",
    "            break\n",
    "\n",
    "    total_reward_history.append(total_reward)\n",
    "    mean_total_reward_in_window = np.mean(total_reward_history[-moving_average_window_size:])\n",
    "    epsilon = update_epsilon(epsilon, train_config)\n",
    "\n",
    "\n",
    "    print(f\"\\rEpisode {episode+1} | Total point average of the last {moving_average_window_size} episodes: {mean_total_reward_in_window:.2f}\", end=\"\")\n",
    "\n",
    "    if (episode+1) % moving_average_window_size == 0:\n",
    "        print(f\"\\rEpisode {episode+1} | Total point average of the last {moving_average_window_size} episodes: {mean_total_reward_in_window:.2f}\")\n",
    "\n",
    "    if (episode+1) % 100 == 0:\n",
    "        network_params_name = f\"params_episode_{episode + 1}\"\n",
    "        params_at_different_training_steps[network_params_name] = train_state.params\n",
    "\n",
    "    # We will consider that the environment is solved if we get an\n",
    "    # average of 200 points in the last 100 episodes.\n",
    "    if mean_total_reward_in_window >= 200.0:\n",
    "        print(f\"\\n\\nEnvironment solved in {episode+1} episodes!\")\n",
    "        network_params_name = f\"params_episode_final\"\n",
    "        params_at_different_training_steps[network_params_name] = train_state.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "\n",
    "def create_video(filename, env, train_state, fps=30):\n",
    "    max_steps = 300\n",
    "    steps = 0\n",
    "    with imageio.get_writer(filename, fps=fps) as video:\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        frame = env.render(mode=\"rgb_array\")\n",
    "        video.append_data(frame)\n",
    "        while not done:    \n",
    "            steps +=1\n",
    "            q_values = network.apply(train_state.params, state)\n",
    "            action = jnp.argmax(q_values)\n",
    "            state, _, done, *_ = env.step(np.asarray([action])[0])\n",
    "            frame = env.render(mode=\"rgb_array\")\n",
    "            video.append_data(frame)\n",
    "            if done and steps < max_steps:\n",
    "                while steps < max_steps:\n",
    "                    video.append_data(frame)\n",
    "                    steps += 1\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params_at_different_training_steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/lunar.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/lunar.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileExistsError\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/lunar.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/lunar.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m episode_name, episode_params \u001b[39min\u001b[39;00m params_at_different_training_steps\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/lunar.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./videos/lunar_\u001b[39m\u001b[39m{\u001b[39;00mepisode_name\u001b[39m}\u001b[39;00m\u001b[39m.gif\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsruinard-machine-learning-novice-w96jq4w7556cgq9r/workspaces/machine_learning_novice/introduction_to_deep_learning_with_jax/lunar.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     episode_train_state \u001b[39m=\u001b[39m TrainingState(episode_params, episode_params, episode_params, initial_opt_state)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'params_at_different_training_steps' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    os.makedirs('./videos')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "for episode_name, episode_params in params_at_different_training_steps.items():\n",
    "    filename = f\"./videos/lunar_{episode_name}.gif\"\n",
    "    episode_train_state = TrainingState(episode_params, episode_params, episode_params, initial_opt_state)\n",
    "    create_video(filename, env, episode_train_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| | | |\n",
    "|:-------------------------:|:-------------------------:|:-------------------------:|\n",
    "|<img width=\"1604\" alt=\"screen shot 2017-08-07 at 12 18 15 pm\" src=\"./videos/lunar_params_episode_100.gif\">  100 Episodes |  <img width=\"1604\" alt=\"screen shot 2017-08-07 at 12 18 15 pm\" src=\"videos/lunar_params_episode_200.gif\"> 200 Episodes|<img width=\"1604\" alt=\"screen shot 2017-08-07 at 12 18 15 pm\" src=\"videos/lunar_params_episode_300.gif\"> 300 Episodes |\n",
    "|<img width=\"1604\" alt=\"screen shot 2017-08-07 at 12 18 15 pm\" src=\"./videos/lunar_params_episode_400.gif\">  400 Episodes |  <img width=\"1604\" alt=\"screen shot 2017-08-07 at 12 18 15 pm\" src=\"videos/lunar_params_episode_500.gif\"> 500 Episodes|<img width=\"1604\" alt=\"screen shot 2017-08-07 at 12 18 15 pm\" src=\"videos/lunar_params_episode_600.gif\"> 600 Episodes |\n",
    "|<img width=\"1604\" alt=\"screen shot 2017-08-07 at 12 18 15 pm\" src=\"./videos/lunar_params_episode_700.gif\">  700 Episodes |  <img width=\"1604\" alt=\"screen shot 2017-08-07 at 12 18 15 pm\" src=\"videos/lunar_params_episode_800.gif\"> 800 Episodes|<img width=\"1604\" alt=\"screen shot 2017-08-07 at 12 18 15 pm\" src=\"videos/lunar_params_episode_final.gif\"> Expert Level Episodes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/tensorflow/__init__.py:29: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils as _distutils\n",
      "2022-08-22 12:15:41.382768: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/keras/utils/image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/keras/utils/image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/keras/utils/image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/keras/utils/image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/keras/utils/image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/keras/utils/image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n",
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/sonnet/src/types.py:34: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  BoolLike = Union[bool, np.bool, TensorLike]\n",
      "2022-08-22 12:15:43.342022: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-22 12:15:43.342059: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['jax_module/linear/b:0',\n",
       " 'jax_module/linear/w:0',\n",
       " 'jax_module/linear_1/b:0',\n",
       " 'jax_module/linear_1/w:0',\n",
       " 'jax_module/linear_2/b:0',\n",
       " 'jax_module/linear_2/w:0']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tree\n",
    "from jax.experimental import jax2tf\n",
    "import sonnet as snt\n",
    "\n",
    "\n",
    "\n",
    "def create_variable(path, value):\n",
    "  name = '/'.join(map(str, path)).replace('~', '_')\n",
    "  return tf.Variable(value, name=name)\n",
    "N_FEATURES_PER_STATE = 8\n",
    "polymorphic_state_shape = jax2tf.shape_poly.PolyShape(\n",
    "  \"b\", N_FEATURES_PER_STATE\n",
    ")\n",
    "\n",
    "class JaxModule(snt.Module):\n",
    "  def __init__(self, params, apply_fn, name=None):\n",
    "    super().__init__(name=name)\n",
    "    self._params = tree.map_structure_with_path(create_variable, params)\n",
    "    # self._apply = jax2tf.convert(lambda p, x: apply_fn(p, x), polymorphic_shapes=[None, \"b, 8\"])\n",
    "    self._apply = jax2tf.convert(lambda p, x: apply_fn(p, x), polymorphic_shapes=[None, polymorphic_state_shape])\n",
    "    self._apply = tf.autograph.experimental.do_not_convert(self._apply)\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    return self._apply(self._params, inputs)\n",
    "\n",
    "\n",
    "# network = hk.without_apply_rng(hk.transform(network_fn))\n",
    "# target_network = hk.without_apply_rng(hk.transform(network_fn))\n",
    "# optimiser = optax.adam(1e-3)\n",
    "# initial_params = network.init(\n",
    "#     jax.random.PRNGKey(seed=0), batch.states)\n",
    "net = JaxModule(train_state.params, network.apply)\n",
    "[v.name for v in net.trainable_variables]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.\n",
      "  leaves, treedef = jax.tree_flatten(tree)\n",
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/haiku/_src/data_structures.py:145: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.\n",
      "  return jax.tree_unflatten(treedef, leaves)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([ 0.46667343, -0.03591029, -0.29230744, -0.16951452], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.apply(initial_params, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function(autograph=False, input_signature=[tf.TensorSpec([None, 8])])\n",
    "def forward(x):\n",
    "  return net(x)\n",
    "\n",
    "to_save = tf.Module()\n",
    "to_save.forward = forward\n",
    "to_save.params = list(net.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/haiku/_src/data_structures.py:144: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.\n",
      "  leaves, treedef = jax.tree_flatten(tree)\n",
      "/workspaces/machine_learning_novice/venv/lib/python3.10/site-packages/haiku/_src/data_structures.py:145: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.\n",
      "  return jax.tree_unflatten(treedef, leaves)\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(to_save, \"./lunar_lander_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load(\"./lunar_lander_model/\")\n",
    "preds = loaded.forward(tf.ones([3, 8]))\n",
    "preds_with_serving_default = loaded.signatures['serving_default'](x=tf.ones([2,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5280086 , -0.14223678, -0.32590032, -0.2560407 ,  0.36866826,\n",
       "       -3.6965518 ,  0.        ,  1.        ], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "loaded = tf.saved_model.load(\"./lunar_lander_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## 6 - Deep Q-Learning\n",
    "\n",
    "In cases where both the state and action space are discrete we can estimate the action-value function iteratively by using the Bellman equation:\n",
    "\n",
    "$$\n",
    "Q_{i+1}(s,a) = R + \\gamma \\max_{a'}Q_i(s',a')\n",
    "$$\n",
    "\n",
    "This iterative method converges to the optimal action-value function $Q^*(s,a)$ as $i\\to\\infty$. This means that the agent just needs to gradually explore the state-action space and keep updating the estimate of $Q(s,a)$ until it converges to the optimal action-value function $Q^*(s,a)$. However, in cases where the state space is continuous it becomes practically impossible to explore the entire state-action space. Consequently, this also makes it practically impossible to gradually estimate $Q(s,a)$ until it converges to $Q^*(s,a)$.\n",
    "\n",
    "In the Deep $Q$-Learning, we solve this problem by using a neural network to estimate the action-value function $Q(s,a)\\approx Q^*(s,a)$. We call this neural network a $Q$-Network and it can be trained by adjusting its weights at each iteration to minimize the mean-squared error in the Bellman equation.\n",
    "\n",
    "Unfortunately, using neural networks in reinforcement learning to estimate action-value functions has proven to be highly unstable. Luckily, there's a couple of techniques that can be employed to avoid instabilities. These techniques consist of using a ***Target Network*** and ***Experience Replay***. We will explore these two techniques in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Target Network\n",
    "\n",
    "We can train the $Q$-Network by adjusting it's weights at each iteration to minimize the mean-squared error in the Bellman equation, where the target values are given by:\n",
    "\n",
    "$$\n",
    "y = R + \\gamma \\max_{a'}Q(s',a';w)\n",
    "$$\n",
    "\n",
    "where $w$ are the weights of the $Q$-Network. This means that we are adjusting the weights $w$ at each iteration to minimize the following error:\n",
    "\n",
    "$$\n",
    "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}Q(s',a'; w)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
    "$$\n",
    "\n",
    "Notice that this forms a problem because the $y$ target is changing on every iteration. Having a constantly moving target can lead to oscillations and instabilities. To avoid this, we can create\n",
    "a separate neural network for generating the $y$ targets. We call this separate neural network the **target $\\hat Q$-Network** and it will have the same architecture as the original $Q$-Network. By using the target $\\hat Q$-Network, the above error becomes:\n",
    "\n",
    "$$\n",
    "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}\\hat{Q}(s',a'; w^-)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
    "$$\n",
    "\n",
    "where $w^-$ and $w$ are the weights the target $\\hat Q$-Network and $Q$-Network, respectively.\n",
    "\n",
    "In practice, we will use the following algorithm: every $C$ time steps we will use the $\\hat Q$-Network to generate the $y$ targets and update the weights of the target $\\hat Q$-Network using the weights of the $Q$-Network. We will update the weights $w^-$ of the the target $\\hat Q$-Network using a **soft update**. This means that we will update the weights $w^-$ using the following rule:\n",
    " \n",
    "$$\n",
    "w^-\\leftarrow \\tau w + (1 - \\tau) w^-\n",
    "$$\n",
    "\n",
    "where $\\tau\\ll 1$. By using the soft update, we are ensuring that the target values, $y$, change slowly, which greatly improves the stability of our learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fd19f60d2496e9c9e375be8bbaf07fa2fff2a8edc3dc6611f1c0b323d41b84b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
