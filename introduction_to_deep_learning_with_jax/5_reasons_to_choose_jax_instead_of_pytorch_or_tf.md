# 5 Reasons why you should choose JAX for your deep learning projects instead of PyTorch or Tensorflow

Tensorflow and PyTorch are two of the most widely used deep learning frameworks out there. PyTorch, coming from Meta, started out as more oriented towards researchers, whereas Tensorflow, backed by Google, was generally more accepted within the industry, but even that has arguably changed in recent times. What can be stated which quite some confidence is that these two giants have put many of the other once popular frameworks to rest (CNTK, caffe, etc). However, a surprising event has happened with JAX entering the field. JAX is the latest arrival and is likely to shake things up quite a bit! So here are 5 reasons why you should JAX for your next deep learning project.

# Backed by Google Brain and Google DeepMind

The industry standard might arguably still be Tensorflow, but PyTorch has taken quite a big chunk out of Tensorflow's marketshare. Discussions on reddit even mention that teams within Google are becoming more hesitant to be using Tensorflow due to it's inconsistent API and that teams are moving towards JAX. Don't believe change is happening? DeepMind, probably one of the most advanced AI companies out there, has standardized on JAX. They build their own framework on top of it called Haiku. In addition, Google Brain started a similar endeavour by creating Flax. So the company who created Tensorflow in the first place is putting more chips behind JAX.

# 5000+ models available on HuggingFace

It's one thing that these big tech companies invest in new technologies. They have plenty of resources to make small bets and see how it will turn out. Sure, but don't expect this to be just a small experiment. There are already over 5000 JAX models hosted on HuggingFace and they even provide JAX as an interface to simplify submitting your models. Why is that important? As DeepMind and Google are pushing the AI frontier further, they'll be increasingly releasing SOTA models in JAX to HuggingFace rather than in Tensorflow. Consequenlty, we'll see more benefits of using JAX because those who can incorporate JAX in their enterprise will gain a competitive edge.

# Easily convert JAX to Tensorflow

But JAX is less mature than Tensorflow and PyTorch you might say. They have a massive ecosystem and mature production-ready features to bring the value from these deep learning models to our organizations, while JAX is still lacking in that regard, right? Well... maybe... JAX provides quite some powerful features. One if which is `from jax.experimental import jax2tf #(finally, a code block ;))`. This enables you to develop your models in JAX, but leverage industry tools such as Tensorflow Serving for model deployment. Not bad!

# Run on CPUs/GPUs/TPUs

You might love the clean API interface of PyTorch. With its consistency and Pythonic style, it's easy to get started with. You have moved beyond the basics and are ready to train one of your first massive models. You upload it to your preferred cloud provider and are already getting excited about the massive speedups. In the end, your friends shared their excitements when they first moved from CPUs to GPUs and you are expecting similar results. You see your model getting loaded (you are logging, right?!), training starts and then... bummer. No significant speedup. Aaargh you forgot to move your model to the almighty GPU (i.e. `model.to(device)`). JAX doesn't need this as it will automatically target CPUs, GPUs and TPUs via its XLA (Accelerated Linear Algebra) backend. So you won't have any code changes to run your models on your backend of choice. Sweet!

# Familiar Numpy-like API

Numpy is one of the most widely used scientific python packages out there. So why isn't it more widely used for deep learning. According to Jake van der Plas, several key features are missing. It can't run on accelerated hardware (GPUs/TPUs), it doesn't provide fast optimization via autodifferentation (the thingy used to train your neural network :). It doesn't compile or fuse your operations and finally it misses some parallelization of data and computation features. Behold, that's exactly what JAX is filling in. So what does that mean? It means that all your efforts investing in learning more about the numpy package just got even more valuable. You can now use the same interface for building models that can translate sentences, answer questions and drive cars.

# Conclusion

PyTorch and Tensorflow are still the most sought after deep learning frameworks out there. They have been so widely adopted that many of the older frameworks simply had to give up. So for a while, these were the only options you had. But that has changed! JAX is still in its infancy, but that doesn't make it unstable. It's widely used by bigtech, there are plently of large scale models out there, it can be operationalized by leveraging tensorflow components and run on the backend of your liking, finally, if you are worry about breaking changes, don't worry too much. With its numpy-like programming interface many issues have already be sorted out and I wouldn't expect too many hickups. Want to know more about it's inner workings and how to get started? check out my other blogpost: `Grokking JAX: How to get started with Deep Learning in Jax` or sign up for my email list [here](https://sruinard.medium.com/subscribe) to get machine learning tips and tricks. Stay positive, stay tuned!
